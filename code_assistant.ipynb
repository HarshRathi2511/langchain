{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\n","import getpass\n","import os\n","\n","\n","def _set_env(var: str):\n","    if not os.environ.get(var):\n","        os.environ[var] = getpass.getpass(f\"{var}: \")\n","\n","\n","_set_env(\"OPENAI_API_KEY\")\n","# _set_env(\"ANTHROPIC_API_KEY\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from bs4 import BeautifulSoup as Soup\n","from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n","\n","# LCEL docs\n","url = \"https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel\"\n","loader = RecursiveUrlLoader(\n","    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",")\n","docs = loader.load()\n","\n","# Sort the list based on the URLs and get the text\n","d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n","d_reversed = list(reversed(d_sorted))\n","concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n","    [doc.page_content for doc in d_reversed]\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["code(prefix='To build a Retrieval-Augmented Generation (RAG) chain in LangChain Expression Language (LCEL), you will need to set up a chain that combines a retriever and a language model (LLM). The retriever will fetch relevant documents based on a query, and the LLM will generate responses based on those documents. Here’s how you can do it:', imports='from langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.retrievers import MyRetriever', code='# Define the retriever\\nretriever = MyRetriever()  # Replace with your specific retriever implementation\\n\\n# Define the LLM\\nllm = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create a prompt template for the LLM\\nprompt_template = ChatPromptTemplate.from_template(\"Given the following documents, answer the question: {question}\")\\n\\n# Build the RAG chain\\nrag_chain = prompt_template | retriever | llm | StrOutputParser()\\n\\n# Example usage\\nquery = \"What are the benefits of using RAG?\"\\nresponse = rag_chain.invoke({\"question\": query})\\nprint(response)')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from pydantic import BaseModel, Field\n","\n","### OpenAI\n","\n","# Grader prompt\n","code_gen_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n","    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n","    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n","    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n","    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n","        ),\n","        (\"placeholder\", \"{messages}\"),\n","    ]\n",")\n","\n","\n","# Data model\n","class code(BaseModel):\n","    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n","\n","    prefix: str = Field(description=\"Description of the problem and approach\")\n","    imports: str = Field(description=\"Code block import statements\")\n","    code: str = Field(description=\"Code block not including import statements\")\n","\n","\n","expt_llm = \"gpt-4o-mini\"\n","llm = ChatOpenAI(temperature=0, model=expt_llm)\n","code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n","question = \"How do I build a RAG chain in LCEL?\"\n","solution = code_gen_chain_oai.invoke(\n","    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('To build a Retrieval-Augmented Generation (RAG) chain in LangChain '\n"," 'Expression Language (LCEL), you will need to set up a chain that combines a '\n"," 'retriever and a language model (LLM). The retriever will fetch relevant '\n"," 'documents based on a query, and the LLM will generate responses based on '\n"," 'those documents. Here’s how you can do it:')\n","('from langchain_core.prompts import ChatPromptTemplate\\n'\n"," 'from langchain_openai import ChatOpenAI\\n'\n"," 'from langchain_core.output_parsers import StrOutputParser\\n'\n"," 'from langchain_core.retrievers import MyRetriever')\n","('# Define the retriever\\n'\n"," 'retriever = MyRetriever()  # Replace with your specific retriever '\n"," 'implementation\\n'\n"," '\\n'\n"," '# Define the LLM\\n'\n"," 'llm = ChatOpenAI(model=\"gpt-4\")\\n'\n"," '\\n'\n"," '# Create a prompt template for the LLM\\n'\n"," 'prompt_template = ChatPromptTemplate.from_template(\"Given the following '\n"," 'documents, answer the question: {question}\")\\n'\n"," '\\n'\n"," '# Build the RAG chain\\n'\n"," 'rag_chain = prompt_template | retriever | llm | StrOutputParser()\\n'\n"," '\\n'\n"," '# Example usage\\n'\n"," 'query = \"What are the benefits of using RAG?\"\\n'\n"," 'response = rag_chain.invoke({\"question\": query})\\n'\n"," 'print(response)')\n"]}],"source":["from pprint import pprint\n","\n","pprint(solution.prefix)\n","pprint(solution.imports)\n","pprint(solution.code)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":2}
