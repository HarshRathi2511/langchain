{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshrathi/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Imports and Setup\n",
    "\n",
    "from typing import Sequence, Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: State Management\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Sequence[BaseMessage]\n",
    "    cycle_count: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Reflection Feedback Model\n",
    "\n",
    "class ReflectionFeedback(BaseModel):\n",
    "    feedback: str = Field(description=\"Critique and recommendations for improvement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Output Generation\n",
    "\n",
    "def generate_output(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Generates the initial output based on the user's query.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with the generated response.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    user_query = messages[0].content\n",
    "\n",
    "    model = ChatOpenAI(temperature=0.7, model=\"gpt-4\")\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Respond to the following query in a clear and structured manner:\\n\\n{query}\",\n",
    "        input_variables=[\"query\"],\n",
    "    )\n",
    "    llm_chain = prompt | model\n",
    "\n",
    "    response = llm_chain.invoke({\"query\": user_query})\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=response)],\n",
    "        \"cycle_count\": state.get(\"cycle_count\", 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Reflection Process\n",
    "\n",
    "def reflect_output(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Reflects on the quality of the generated output.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with the reflection feedback.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    generated_output = messages[-1].content\n",
    "\n",
    "    reflection_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an expert reviewer. Critique the following output based on structure, clarity, depth, and relevance. Provide specific recommendations for improvement.\n",
    "\n",
    "Output:\n",
    "{output}\n",
    "\n",
    "Critique:\"\"\",\n",
    "        input_variables=[\"output\"],\n",
    "    )\n",
    "\n",
    "    model = ChatOpenAI(temperature=0.5, model=\"gpt-4\")\n",
    "    llm_chain = reflection_prompt | model.with_structured_output(ReflectionFeedback)\n",
    "\n",
    "    reflection = llm_chain.invoke({\"output\": generated_output})\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=reflection.feedback)],\n",
    "        \"cycle_count\": state.get(\"cycle_count\", 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Revision Process\n",
    "\n",
    "def revise_output(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Revises the generated output based on the reflection feedback.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with the revised response.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    generated_output = messages[-2].content  # Original output\n",
    "    feedback = messages[-1].content        # Reflection feedback\n",
    "    cycle_count = state.get(\"cycle_count\", 0)\n",
    "\n",
    "    revision_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are tasked with revising the following output based on the provided feedback.\n",
    "\n",
    "Original Output:\n",
    "{output}\n",
    "\n",
    "Feedback:\n",
    "{feedback}\n",
    "\n",
    "Revised Output:\"\"\",\n",
    "        input_variables=[\"output\", \"feedback\"],\n",
    "    )\n",
    "\n",
    "    model = ChatOpenAI(temperature=0.5, model=\"gpt-4\")\n",
    "    llm_chain = revision_prompt | model\n",
    "\n",
    "    revised_output = llm_chain.invoke({\"output\": generated_output, \"feedback\": feedback})\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=revised_output)],\n",
    "        \"cycle_count\": cycle_count + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StateGraph' object has no attribute 'add_conditional_edge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_conditional_edge\u001b[49m(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m state: state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcycle_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_conditional_edge(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     END,\n\u001b[1;32m     21\u001b[0m     condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m state: state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcycle_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compile the graph\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StateGraph' object has no attribute 'add_conditional_edge'"
     ]
    }
   ],
   "source": [
    "# Step 6: Define the Workflow with Should Continue State\n",
    "\n",
    "# Define the workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"generate\", generate_output)\n",
    "workflow.add_node(\"reflect\", reflect_output)\n",
    "workflow.add_node(\"revise\", revise_output)\n",
    "workflow.add_node(\"should_continue\", should_continue)  # New node\n",
    "\n",
    "# Define edges\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"reflect\")\n",
    "workflow.add_edge(\"reflect\", \"revise\")\n",
    "workflow.add_edge(\"revise\", \"should_continue\")  # Updated edge to route through should_continue\n",
    "\n",
    "# Define conditional edges from should_continue\n",
    "workflow.add_conditional_edge(\n",
    "    \"should_continue\",\n",
    "    \"generate\",\n",
    "    condition=lambda state: state[\"cycle_count\"] < 3\n",
    ")\n",
    "workflow.add_conditional_edge(\n",
    "    \"should_continue\",\n",
    "    END,\n",
    "    condition=lambda state: state[\"cycle_count\"] >= 3\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Should Continue State\n",
    "\n",
    "def should_continue(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Determines whether the workflow should continue based on the cycle count.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with the next node to transition to.\n",
    "    \"\"\"\n",
    "    cycle_count = state.get(\"cycle_count\", 0)\n",
    "    if cycle_count < 3:\n",
    "        next_node = \"generate\"\n",
    "    else:\n",
    "        next_node = END\n",
    "\n",
    "    return {\n",
    "        \"next_node\": next_node,\n",
    "        \"cycle_count\": cycle_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      8\u001b[0m         HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite an essay on the impact of social media on modern communication.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m     ],\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcycle_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     15\u001b[0m         pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput from node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 8: Execution Example\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Write an essay on the impact of social media on modern communication.\"),\n",
    "    ],\n",
    "    \"cycle_count\": 0\n",
    "}\n",
    "\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization requires additional dependencies.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Visualization (Optional)\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print(\"Visualization requires additional dependencies.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
